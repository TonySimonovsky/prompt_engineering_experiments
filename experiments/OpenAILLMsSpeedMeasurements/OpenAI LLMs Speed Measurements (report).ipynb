{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22441125-7eae-4d13-b8c9-97a8fe3704f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d590fa84-64bd-49a0-8dff-72e27d7f923f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../../../settings/.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b3d790-68e6-4c13-9c7c-93f34f952bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5707ca13-b8ae-4f02-9979-bda500e68c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../../../../05 Own Solutions/AIChampTools')\n",
    "\n",
    "import importlib\n",
    "\n",
    "from AIChampTools import AIChampTools, LLMUsage, PromptEngineeringExperiment\n",
    "prevent_output = importlib.reload(sys.modules['AIChampTools'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b858d31a-47d8-48e2-b575-613d4237bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = PromptEngineeringExperiment(\n",
    "    name=\"OpenAI_LLMs_Speed_Measurements\",\n",
    "    logs_folder=\"../../logs/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9803c0-3782-49aa-a476-262bf18dd9f1",
   "metadata": {},
   "source": [
    "# Experiment Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc73242-a628-4203-be9c-bab85c18ed23",
   "metadata": {},
   "source": [
    "In this experiment I will be regularly monitoring time required to get a completion for different OpenAI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f89fb-1630-4105-8902-8748502eb883",
   "metadata": {},
   "source": [
    "## Version 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "643b1259-9db7-4f00-ae9b-88b356531d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Variation 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Messages template**: [\"[{'role': 'user', 'content': '{user_message}'}]\"]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User Messages**: [\"{'user_message': 'Explain data science to a CEO without any knowledge in it.'}\"]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Params**: {\"model\": \"gpt-3.5-turbo\", \"temperature\": 0, \"timeout\": 30}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Completions in the experiment**: 31"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Execution times stats**: count    31.000000\n",
       "mean     10.261553\n",
       "std       0.970875\n",
       "min       8.566172\n",
       "25%       9.544703\n",
       "50%      10.082722\n",
       "75%      11.136921\n",
       "max      12.268645\n",
       "Name: generation_time, dtype: float64"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Variation 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Messages template**: [\"[{'role': 'user', 'content': '{user_message}'}]\"]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**User Messages**: [\"{'user_message': 'Explain data science to a CEO without any knowledge in it.'}\"]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**LLM Params**: {\"model\": \"gpt-3.5-turbo-1106\", \"max_tokens\": 200, \"temperature\": 0, \"timeout\": 30}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Completions in the experiment**: 31"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Execution times stats**: count    31.000000\n",
       "mean      3.650511\n",
       "std       0.947394\n",
       "min       2.250462\n",
       "25%       2.849933\n",
       "50%       3.720609\n",
       "75%       4.236674\n",
       "max       5.585114\n",
       "Name: generation_time, dtype: float64"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ver01_results_nf = experiment.load_results(ver=\"01\")\n",
    "\n",
    "ver01_results_nf[\"llm_params\"] = ver01_results_nf[\"llm_params\"].apply(json.dumps)\n",
    "ver01_results_nf[\"messages_template\"] = ver01_results_nf[\"messages_template\"].apply(str)\n",
    "\n",
    "for i, llm_params in enumerate(ver01_results_nf[\"llm_params\"].unique()):\n",
    "    completions = ver01_results_nf[ver01_results_nf[\"llm_params\"]==llm_params]\n",
    "    usage_df = pd.json_normalize(completions['llm_usage'])\n",
    "    \n",
    "    printmd(f'### Variation {i+1}')\n",
    "    printmd(f'**Messages template**: {list(completions[\"messages_template\"].unique())}')\n",
    "    printmd(f'**User Messages**: {completions[\"data\"].apply(str).unique()}')\n",
    "    printmd(f\"**LLM Params**: {llm_params}\")\n",
    "    printmd(f'**Completions in the experiment**: {len(completions)}')\n",
    "    printmd(f'**Execution times stats**: {usage_df[\"generation_time\"].describe()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
